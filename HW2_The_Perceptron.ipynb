{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "2ca2fadc-7bdb-413d-ffd5-895598611f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-07 04:07:52--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "test.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 04:07:52 (549 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n",
            "--2022-02-07 04:07:52--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 04:07:52 (354 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "bf09cfed-d789-4add-d5ce-0c9a867cfbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = [list(map(int, instance.strip().split('\\t')))]\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        #instance = [-1] + instance\n",
        "        data += instance\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    dp = 0\n",
        "    for i in range(len(array1)):\n",
        "        dp += (array1[i]*array2[i])\n",
        "    return dp"
      ],
      "metadata": {
        "id": "JhOfQPoKqPQk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    return 1 / (1 + math.exp(-x))\n"
      ],
      "metadata": {
        "id": "i2YJo69bqMgP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    #TODO: return the output of the model \n",
        "    op = 0\n",
        "    for j in range(len(instance)):\n",
        "        op += (instance*weight)\n",
        "    return op+sigmoid(x)\n"
      ],
      "metadata": {
        "id": "l9XPFrcmqG5J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "\n",
        "def predict(weight, instance):\n",
        "  #TODO: return the prediction of the model\n",
        "  activation =weight[0]\n",
        "  for i in range(len(instance)-1):\n",
        "\t  activation += weight[i + 1] * instance[i]\n",
        "\t  return 1 if activation >= 0.5 else 0\n"
      ],
      "metadata": {
        "id": "LPMigLZbqECE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n"
      ],
      "metadata": {
        "id": "o9fd43TGqBbp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: name this step\n",
        "    #Initilizing weights\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: name these steps\n",
        "            #calculate dot product and make same as Input to sigmoid \n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: name these steps\n",
        "            #Updating weights\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "881o-K42p701"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a647f23-35b7-460d-f2d7-4d36adcf8ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "**Answer:**\n",
        "It is a perceptron model. For this input to sigmoid will be the dot product of input. At the same time, the alternate code written above predicts output without sigmoid and activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c31ee7-10e0-4fce-88d5-9284d994ce2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 42.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 42.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 42.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 32.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 42.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "\n",
        "**As per the instructions provided, the results are as follows with the different hyperparameters, and the Learning rate makes more efficient in the accuracy. The training percentage also matters for the initial epochs.\n",
        "An increase in the epochs will not result in increased accuracy.\n",
        "Calculated the code in the three different combinations,\n",
        "Using default parameters, Hyperparameters optimization, and Learning Rate and Percentage with 5 to 6 epochs.**\n",
        "\n",
        "\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "**Answer: **There is no need for training with all the training data set, As we can see in the following results with 20 training data, we are getting an accuracy of 68.0, and it is unchanged with 400 training data.\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "**Answer:**\n",
        "The training (epoch) is organized as batches of data so that the optimization function is calculated within the subset of the whole dataset. The console output shows the accuracy of the entire dataset, so when we try to optimize a single batch, thus results in the decrease of the accuracy of the other part of the dataset and decreases the global result.\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "**Answer:**\n",
        "We cannot straight away deny this, and there can be the chance of getting >80.0 with the combination of different hyperparameter\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "**Answers:**\n",
        "When there is a rise in the number of epochs, it reflects in the increase in weights changed in Neural Network, and then the curve goes from underfitting to optimal to overfitting.\n",
        "Consider the example: The model will miss some of the required patterns if low learning rate. If it is high, it may have collisions.\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [50] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "      for tr_size in tr_percent:\n",
        "        for epochs in num_epochs:\n",
        "          size =  round(len(instances_tr)*tr_size/100)\n",
        "          pre_instances = instances_tr[0:size]\n",
        "          weights = train_perceptron(pre_instances, lr, epochs)\n",
        "          accuracy = get_accuracy(weights, instances_te)\n",
        "        print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n",
        "\n",
        "\"\"\"after running this code we can observe that, epocs not make much difference as compair to other parameters \n",
        "like training percent and learning rate\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Z-c2VPvzy0UJ",
        "outputId": "023111de-c0ec-4900-989e-41591286894c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'after running this code we can observe that, epocs not make much difference as compair to other parameters \\nlike training percent and learning rate'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2_The_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}